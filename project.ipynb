{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load packages env py_dl\n",
    "#racheli\n",
    "import os\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, regularization\n",
    "from keras.layers import Embedding, Reshape, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "os.environ['OMP_NUM_THREADS'] = '50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) constants and hyperparameters\n",
    "L_RBNS = 20 # length of each sequence in RBNS data\n",
    "O = int(1e7) # for initializing big arrays, helps reduce runtime\n",
    "LIMIT_FILE_N_SEQ_READ = int(1e6) # limit the amount of seq we read from file, helps reduce runtime\n",
    "ONE_HOT_DICT = {b'A': np.array([0,0,0,1], dtype=np.float16),\n",
    "                b'C': np.array([0.,0.,1,0], dtype=np.float16),\n",
    "                b'G': np.array([0,1,0,0], dtype=np.float16),\n",
    "                b'T': np.array([1,0,0,0], dtype=np.float16),\n",
    "                b'U': np.array([1,0,0,0], dtype=np.float16),\n",
    "                b'N': np.array([0.25,0.25,0.25,0.25])\n",
    "                }\n",
    "\n",
    "ONE_HOT_DICT2 = {'A':[0,0,0,1],\n",
    "                'C': [0,0,1,0],\n",
    "                'G':[0,1,0,0],\n",
    "                'T': [1,0,0,0],\n",
    "                'U': [1,0,0,0],\n",
    "                'N': [0.25,0.25,0.25,0.25]\n",
    "                }\n",
    "\n",
    "L_RBNS = 20 # length of each sequence in RBNS data\n",
    "FILES_40 = [\"RBP9\" , \"RBP11\", \"RBP15\", \"RBP31\"] # files  with sequences len 40 and not 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBP1_1300nM.seq', 'RBP1_20nM.seq', 'RBP1_320nM.seq', 'RBP1_5nM.seq', 'RBP1_80nM.seq', 'RBP1_input.seq']\n"
     ]
    }
   ],
   "source": [
    "# 3) read files \n",
    "# list of files in RBNS_training\n",
    "directory = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training'\n",
    "RBNS_training_files = []\n",
    "for filename in os.listdir(directory): # Iterate over all files in the directory\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        RBNS_training_files.append(filename)\n",
    "# print(RBNS_training_files)\n",
    "\n",
    "# filter out protein 1 files only\n",
    "PROTEIN = \"RBP1\" ### TODO change \n",
    "filtered_list = [value for value in RBNS_training_files if value.startswith(str(PROTEIN)+'_')]\n",
    "print(filtered_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBP1_input.seq', 'RBP1_5nM.seq', 'RBP1_20nM.seq', 'RBP1_80nM.seq', 'RBP1_320nM.seq', 'RBP1_1300nM.seq']\n"
     ]
    }
   ],
   "source": [
    "modified_list = [value.replace('RBP1_', '').replace('nM.seq', '') for value in filtered_list]\n",
    "\n",
    "if 'RBP1_input.seq' in filtered_list: # get rid of input\n",
    "    modified_list.remove('input.seq')\n",
    "modified_list = sorted(modified_list, key=int)\n",
    "# order file names according to concentration\n",
    "prefixes = [item.split('_', 1)[0] for item in filtered_list]\n",
    "suffixes = [item.split('_', 1)[1] for item in filtered_list]\n",
    "modified_list = [PROTEIN+\"_\"+str(modified_list[i])+\"nM.seq\" for i in range(len(modified_list))]\n",
    "modified_list.insert(0,str(PROTEIN)+\"_input.seq\")\n",
    "print(modified_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4) BINARY classification - \n",
    "# prefixes = [item.split('_', 1)[0] for item in filtered_list]\n",
    "# suffixes = [item.split('_', 1)[1] for item in filtered_list]\n",
    "\n",
    "# # find the middle affinity files\n",
    "# modified_list = [value.replace('RBP1_', '').replace('nM.seq', '') for value in filtered_list]\n",
    "# if 'RBP1_input.seq' in filtered_list: # get rid of input\n",
    "#     modified_list.remove('input.seq')\n",
    "# modified_list = sorted(modified_list, key=int)\n",
    "\n",
    "# # modified_list = [PROTEIN + \"_\" + value + \"nM.seq\" for value in modified_list[1:3]]  # the first 2 files\n",
    "# modified_list = [PROTEIN + \"_\" + value + \"nM.seq\" for value in [modified_list[1], modified_list[-2]]]  # the first 2 files\n",
    "# modified_list.append(str(PROTEIN+'_input.seq')) # 0\n",
    "# modified_list\n",
    "\n",
    "# # initalize np.arrays\n",
    "# master_list = np.empty((O), dtype=f'|S{L_RBNS}') # sequences\n",
    "# class_lables = np.zeros((O, len(modified_list))) # array of probolities per class\n",
    "# n = 0\n",
    "\n",
    "# ## running on protein 1 files only ############################################################\n",
    "# for file_index, file in enumerate(modified_list):\n",
    "#     #print(file_index)\n",
    "#     file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/' + file\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         # choose sequences from file randomly - limited to LIMIT_FILE_N_SEQ_READ\n",
    "#         rng = np.random.default_rng(seed=123)\n",
    "#         rand_indices = rng.choice(len(lines), size=LIMIT_FILE_N_SEQ_READ, replace=False)\n",
    "\n",
    "#         # cut seq to 20 length randomly if len is longer find a random starting index and take 20 from there\n",
    "#         if file in FILES_40: # file len 40\n",
    "#             # choose random index to start from and select 20 chars - save shortend seq into master_list\n",
    "#             start_index = random.randint(0, 20)\n",
    "#             for index in rand_indices:\n",
    "#                 seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                 master_list[n] = seq[start_index:start_index+20] # shortened seq\n",
    "#                 if file == str(PROTEIN+'_input.seq'):\n",
    "#                     class_lables[n, file_index] = 0\n",
    "#                 else:\n",
    "#                     class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "#                 n += 1\n",
    "#         elif file == str(PROTEIN+'_input.seq'):\n",
    "#               for index in rand_indices:\n",
    "#                     seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                     master_list[n] = seq # seq\n",
    "#                     class_lables[n, file_index] = 0 # lables - 1 if in file otherwise stays 0\n",
    "#                     n += 1\n",
    "#         else: # file len 20 + isnt input.seq\n",
    "#             for index in rand_indices:\n",
    "#                     seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                     master_list[n] = seq # seq\n",
    "#                     class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "#                     n += 1\n",
    "# # free memory initlized that wasnt used \n",
    "# master_list = master_list[:n]\n",
    "# class_lables = class_lables[:n]\n",
    "# del lines\n",
    "\n",
    "# # binary vec\n",
    "# class_lables_binary = np.where(((class_lables[:, 0] == 1) | (class_lables[:, 1] == 1)) & (class_lables[:, 2] == 0), 1, 0)\n",
    "# class_lables_binary\n",
    "\n",
    "# # convert master_list seqences to one hot encoding\n",
    "# one_hot = np.zeros((len(master_list),  L_RBNS, 4), dtype=np.float16)\n",
    "# one_hot = np.array([ONE_HOT_DICT[bytes([nuc])] for seq in master_list for nuc in seq])\n",
    "# one_hot = one_hot.reshape(len(master_list), -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) MULTICALSS classification - \n",
    "# initalize np.arrays\n",
    "master_list = np.empty((O), dtype=f'|S{L_RBNS}') # sequences\n",
    "class_lables = np.zeros((O, len(modified_list))) # array of probolities per class\n",
    "n = 0\n",
    "\n",
    "## running on protein 1 files only ############################################################\n",
    "for file_index, file in enumerate(modified_list):\n",
    "    #print(file_index)\n",
    "    file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/' + file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # choose sequences from file randomly - limited to LIMIT_FILE_N_SEQ_READ\n",
    "        rng = np.random.default_rng(seed=123)\n",
    "        rand_indices = rng.choice(len(lines), size=LIMIT_FILE_N_SEQ_READ, replace=False)\n",
    "\n",
    "        # cut seq to 20 length randomly if len is longer find a random starting index and take 20 from there\n",
    "        if file in FILES_40: # file len 40\n",
    "            # choose random index to start from and select 20 chars - save shortend seq into master_list\n",
    "            start_index = random.randint(0, 20)\n",
    "            for index in rand_indices:\n",
    "                seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "                master_list[n] = seq[start_index:start_index+20] # shortened seq\n",
    "                if file == str(PROTEIN+'_input.seq'):\n",
    "                    class_lables[n, file_index] = 0\n",
    "                else:\n",
    "                    class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "                n += 1\n",
    "        elif file == str(PROTEIN+'_input.seq'):\n",
    "              for index in rand_indices:\n",
    "                    seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "                    master_list[n] = seq # seq\n",
    "                    class_lables[n, file_index] = 0 # lables - 1 if in file otherwise stays 0\n",
    "                    n += 1\n",
    "        else: # file len 20 + isnt input.seq\n",
    "            for index in rand_indices:\n",
    "                    seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "                    master_list[n] = seq # seq\n",
    "                    class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "                    n += 1\n",
    "# free memory initlized that wasnt used \n",
    "master_list = master_list[:n]\n",
    "class_lables = class_lables[:n]\n",
    "del lines\n",
    "\n",
    "# convert master_list seqences to one hot encoding\n",
    "one_hot = np.zeros((len(master_list),  L_RBNS, 4), dtype=np.float16)\n",
    "one_hot = np.array([ONE_HOT_DICT[bytes([nuc])] for seq in master_list for nuc in seq])\n",
    "one_hot = one_hot.reshape(len(master_list), -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # back up: MULTICALSS = initalize np.arrays\n",
    "# master_list = np.empty((O), dtype=f'|S{L_RBNS}') # sequences\n",
    "# # class_lables = np.zeros((O, len(filtered_list)), dtype=np.bool_) # array of probolities per class\n",
    "# class_lables = np.zeros((O, len(filtered_list))) # array of probolities per class\n",
    "# n = 0\n",
    "\n",
    "# ## running on protein 1 files only ############################################################\n",
    "# for file_index, file in enumerate(filtered_list):\n",
    "#     #print(file_index)\n",
    "#     file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/' + file\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "#         # choose sequences from file randomly - limited to LIMIT_FILE_N_SEQ_READ\n",
    "#         rng = np.random.default_rng(seed=123)\n",
    "#         rand_indices = rng.choice(len(lines), size=LIMIT_FILE_N_SEQ_READ, replace=False)\n",
    "\n",
    "#         # cut seq to 20 length randomly if len is longer find a random starting index and take 20 from there\n",
    "#         if file in FILES_40: # file len 40\n",
    "#             # choose random index to start from and select 20 chars - save shortend seq into master_list\n",
    "#             start_index = random.randint(0, 20)\n",
    "#             for index in rand_indices:\n",
    "#                 seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                 master_list[n] = seq[start_index:start_index+20] # shortened seq\n",
    "#                 class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "#                 n += 1\n",
    "#         else: # file len 20\n",
    "#             for index in rand_indices:\n",
    "#                     seq = lines[index].split('\\t')[0] # take only the sequence\n",
    "#                     master_list[n] = seq # seq\n",
    "#                     class_lables[n, file_index] = 1 # lables - 1 if in file otherwise stays 0\n",
    "#                     n += 1\n",
    "# # free memory initlized that wasnt used \n",
    "# master_list = master_list[:n]\n",
    "# class_lables = class_lables[:n]\n",
    "# del lines\n",
    "# # # convert master_list seqences to one hot encoding\n",
    "# one_hot = np.zeros((len(master_list),  L_RBNS, 4), dtype=np.float16)\n",
    "# one_hot = np.array([ONE_HOT_DICT[bytes([nuc])] for seq in master_list for nuc in seq])\n",
    "# one_hot = one_hot.reshape(len(master_list), -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000000,) (6000000, 6) (6000000, 20, 4)\n",
      "(4800000,) (4800000, 6) (1200000,) (1200000, 6)\n"
     ]
    }
   ],
   "source": [
    "# 5) split into training and validation sets -- 80% training, 20% validation\n",
    "# TODO LATER: keep out 6~ proteins for validation of the end\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "one_hot_train_X, one_hot_valid_X, one_hot_train_Y, one_hot_valid_Y = train_test_split(master_list, class_lables, test_size=0.2, random_state=42)\n",
    "print(master_list.shape,class_lables.shape, one_hot.shape)\n",
    "print(one_hot_train_X.shape, one_hot_train_Y.shape, one_hot_valid_X.shape, one_hot_valid_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the layer size is: 32\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, 18, 32)            416       \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 9, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                9248      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,862\n",
      "Trainable params: 9,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 6) NN per protien - input each seq (len 20) in master_list into NN \n",
    "    # output = vec of probabilities (one per each concentration)\n",
    "    # compare output to bool numpy array (true lable)\n",
    "    # backpropogation to model to minimize loss?\n",
    "# X=master_list\n",
    "# Y=class_lables\n",
    "model_param_dict = {\"kernel_size\":3, \"pool_size\":2, \"layers\": [32], \"final_activation_function\":\"softmax\"}\n",
    "output_size=6 # number of classes\n",
    "X = one_hot\n",
    "Y=class_lables\n",
    "\n",
    "# # first try - simple NN\n",
    "# model_1=Sequential()\n",
    "# model_1.add(Dense(11, activation=\"relu\",input_shape=one_hot_train_X.shape[1:]))\n",
    "# model_1.add(Flatten())\n",
    "# model_1.add(Dense(output_size, activation=model_param_dict['final_activation_function'])) # softmax\n",
    "\n",
    "# second try - CNN\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=model_param_dict[\"kernel_size\"], strides=1,\\\n",
    "                 kernel_initializer=initializers.RandomNormal(stddev=0.01), \\\n",
    "                 activation='relu',\\\n",
    "                 input_shape=X.shape[1:], use_bias=True, bias_initializer='RandomNormal'))\n",
    "model.add(MaxPooling1D(pool_size=model_param_dict[\"pool_size\"], strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Flatten())\n",
    "# per layer\n",
    "for layer_size in model_param_dict['layers']:\n",
    "            print(f'the layer size is: {layer_size}')\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "model.add(Dense(output_size, activation=model_param_dict['final_activation_function'])) \n",
    "\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# Hyperparameters\n",
    "EPHOCHS = 1\n",
    "model.compile(optimizer=Adam(learning_rate = .003), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13438/32813 [===========>..................] - ETA: 10:45 - loss: 1.5113 - accuracy: 0.2485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m y_train \u001b[39m=\u001b[39m Y\n\u001b[1;32m      6\u001b[0m \u001b[39m# Compile the model with Optimizer, Loss Function and Metrics\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m run_hist_1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X, Y, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, epochs\u001b[39m=\u001b[39mEPHOCHS, validation_split\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function\u001b[39m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate dummy data for training\n",
    "\n",
    "X_train = X\n",
    "y_train = Y\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "run_hist_1 = model.fit(X, Y, batch_size=128, epochs=EPHOCHS, validation_split=0.3, shuffle=True, use_multiprocessing=True, workers=90)\n",
    "model.save('/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5') # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 11:27:39.382810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 11:27:39.398012: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 3. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = keras.models.load_model(\"/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_sig_80_320.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - hyperparameter tuning\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# hyperparameters to play with:\n",
    "# 1) number of layers -- 1, 2, 3\n",
    "# 2) number of neurons per layer -- 32, 64, 128\n",
    "# 3) activation function -- \"relu\", \"sigmoid\", \"tanh\"\n",
    "# 4) optimizer -- \"adam\", \"sgd\", \"rmsprop\"\n",
    "# 5) learning rate -- 0.001, 0.01, 0.1\n",
    "# 6) batch size -- 8, 16, 32\n",
    "# 7) number of epochs -- 2, 4, 8\n",
    "# 8) kernel size -- 3, 5, 7\n",
    "# 9) pool size -- 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) TRYING TO RE-BUILD SHIFTS FOR PRED\n",
    "# cut seg from RNAcomplete file into all shifts of len 20\n",
    "# read txt file\n",
    "file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNAcompete_sequences.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # convert seqences to one hot encoding + pad\n",
    "    RNAcompete_master_list_one_hot = [[ONE_HOT_DICT2[nuc] for nuc in ((41 - len(seq.rstrip('\\n'))) * 'N' + seq.rstrip('\\n'))] for seq in file if len(seq) > 5]\n",
    "num_seqs = len(RNAcompete_master_list_one_hot)\n",
    "\n",
    "# SHIFTS - ut seg from RNAcomplete file into all shifts of len 20 -- create k-mers of all possible shifts \n",
    "shifts_RNAcompete_master_list = [[seq[i:i+20] for i in range(0, len(seq) - 20 + 1, 5)] for seq in RNAcompete_master_list_one_hot] # step +5 - 41,4 > 20,4\n",
    "shifts_RNAcompete_master_list = np.reshape(shifts_RNAcompete_master_list, (-1, 20, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206785, 20, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifts_RNAcompete_master_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37713/37713 [==============================] - 37s 973us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(241357,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize the prediction + output ##################################################################################################################\n",
    "\n",
    "# predict for one datapoint is scalar => prediction.shape = (num_seqs, -1)\n",
    "# predict for one datapoint is vector N dim => prediction.shape = (num_seqs, -1, N)\n",
    "################################################\n",
    "prediction = model.predict(shifts_RNAcompete_master_list, use_multiprocessing=True, workers=90)\n",
    "prediction = np.reshape(prediction, (num_seqs, -1))\n",
    "max_prediction = np.max(prediction, axis=1)\n",
    "max_prediction.shape\n",
    "\n",
    "# write pred into file\n",
    "protein=\"RBP1\"\n",
    "with open(f'/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/max_predicitions_{protein}.txt', \"w\") as file:\n",
    "    for pred in max_prediction:\n",
    "        file.write(f'{pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR MULTICLASS -  suppose predict gives N dim vector:\n",
    "# prediction = model.predict(shifts_RNAcompete_master_list, use_multiprocessing=True, workers=90)\n",
    "# prediction = np.reshape(prediction, (num_seqs, -1, N))\n",
    "# features = np.empty((num_seqs, 2 * N))\n",
    "# features[:, :N] = np.max(prediction, axis=1) # shape is (num_seqs, N)\n",
    "# features[:, N:] = np.min(prediction, axis=1) # shape is (num_seqs, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.05204342628474394 p-value: 2.2321120254400805e-144\n"
     ]
    }
   ],
   "source": [
    "# reloud model + read pred + check correlation\n",
    "import scipy.stats as stats\n",
    "with open('/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNCMPT_training/RBP1.txt','r') as file:\n",
    "    y_target=[float(line.strip()) for line in file] ### y\n",
    "       \n",
    "with open('/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/max_predicitions_RBP1.txt','r') as file:\n",
    "    y_hat=[float(line.strip('[]\\n')) for line in file] ### x\n",
    "\n",
    "correlation_coefficient, p_value = stats.pearsonr(y_target, y_hat)\n",
    "print(\"Pearson correlation coefficient:\", correlation_coefficient, \"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [91.57608154]\n",
      "Intercept: -59.67642204302061\n",
      "Maximum: 1.372548667368136\n",
      "Minimum: 0.2073307653064873\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LinearRegression - between y_hat and y_target\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array(y_hat).reshape(-1, 1)  # Independent variable (features)\n",
    "y = np.array(y_target)  # Dependent variable (target)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Print the coefficients (slope and intercept)\n",
    "print(\"Coefficients:\", model.coef_)     # Slope\n",
    "print(\"Intercept:\", model.intercept_)   # Intercept\n",
    "print(\"Maximum:\", max(y_pred))\n",
    "print(\"Minimum:\", min(y_pred))\n",
    "\n",
    "# Visualize the linear regression line\n",
    "plt.scatter(X, y) \n",
    "plt.plot(X, y_pred, color='red', label='Linear Regression') ## model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.6634457 ],\n",
       "        [0.64907414]], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # test -- one seq from input file -- should calssify as 0 - check!\n",
    "\n",
    "# file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/RBP1_input.txt'\n",
    "# with open(file_path, 'r') as file:\n",
    "#     first_line = file.readline()\n",
    "#     second_line = file.readline()\n",
    "\n",
    "# # Extract the first element from both lines\n",
    "# seq1 = first_line.split('\\t')[0]\n",
    "# seq2 = second_line.split('\\t')[0]\n",
    "# # print(seq1)\n",
    "\n",
    "# one_hot_test_1 = np.array([ONE_HOT_DICT2[(nuc)] for nuc in seq1])\n",
    "# one_hot_test_2 = np.array([ONE_HOT_DICT2[(nuc)] for nuc in seq2])\n",
    "# # print(one_hot_test_1.shape)\n",
    "# # print(one_hot_test_1)\n",
    "# one_hot_test = np.array([one_hot_test_1, one_hot_test_2])\n",
    "# # one_hot_test.shape\n",
    "\n",
    "# # print(one_hot_test)\n",
    "# # print(one_hot_test.shape)\n",
    "# # model.predict(one_hot_test)\n",
    "\n",
    "# # create prediction per seq - take max pred 1 seq\n",
    "# predictions = [model.predict(one_hot_test)]\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 4)\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.67058665],\n",
       "        [0.67228156]], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # test --  one seq from input file -- should calssify as 1 - check!\n",
    "\n",
    "# file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/RBP1_80nM.seq'\n",
    "# with open(file_path, 'r') as file:\n",
    "#     first_line = file.readline()\n",
    "#     second_line = file.readline()\n",
    "\n",
    "# # Extract the first element from both lines\n",
    "# seq1 = first_line.split('\\t')[0]\n",
    "# seq2 = second_line.split('\\t')[0]\n",
    "# # print(seq1)\n",
    "\n",
    "# one_hot_test_1 = np.array([ONE_HOT_DICT2[(nuc)] for nuc in seq1])\n",
    "# one_hot_test_2 = np.array([ONE_HOT_DICT2[(nuc)] for nuc in seq2])\n",
    "# print(one_hot_test_1.shape)\n",
    "# # print(one_hot_test_1)\n",
    "# one_hot_test = np.array([one_hot_test_1, one_hot_test_2])\n",
    "# # print(one_hot_test.shape)\n",
    "# # print(one_hot_test)\n",
    "\n",
    "# # print(one_hot_test)\n",
    "# # print(one_hot_test.shape)\n",
    "# # model.predict(one_hot_test)\n",
    "\n",
    "# # # create prediction per seq - take max pred 1 seq\n",
    "# predictions = [model.predict(one_hot_test)]\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # cut seg from RNAcomplete file into all shifts of len 20\n",
    "# # RNAcompete_master_list_one_hot = np.empty((O), dtype=f'|S{L_RBNS*2}') # sequences\n",
    "\n",
    "# # # read txt file\n",
    "# # file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNAcompete_sequences.txt'\n",
    "# # with open(file_path, 'r') as file:\n",
    "# #     # convert seqences to one hot encoding\n",
    "# #     RNAcompete_master_list_one_hot = [[ONE_HOT_DICT2[nuc] for nuc in seq.rstrip('\\n')] for seq in file]\n",
    "\n",
    "# # cut seg from RNAcomplete file into all shifts of len 20\n",
    "# shifts_RNAcompete_master_list = np.empty((O), dtype=object) # sequences\n",
    "\n",
    "# for n, seq_onehot in enumerate(RNAcompete_master_list_one_hot): # seq\n",
    "#     line_length = len(seq_onehot)\n",
    "#     if line_length > 20: # create k-mers of all possible shifts \n",
    "#         shifts = np.empty((line_length - 20), dtype=object)\n",
    "#         j=0\n",
    "#         if j <= 6:\n",
    "#             for i in range(line_length - 20): # shift\n",
    "#                 shifts[i] = np.array(seq_onehot[j:j+20])\n",
    "#                 j+=3\n",
    "#         shifts_RNAcompete_master_list[n] = shifts\n",
    "#     else:\n",
    "#         shifts_RNAcompete_master_list[n] = seq_onehot\n",
    "                \n",
    "# # shifts_RNAcompete_master_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241357, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list to 2d df\n",
    "# pd.DataFrame(y_hat)\n",
    "np.array(y_hat).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "endog must be in the unit interval.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39madd_constant(df)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Create a logistic regression model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logit_model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mLogit(y_target, y_pred)\n\u001b[1;32m     12\u001b[0m logit_result \u001b[39m=\u001b[39m logit_model\u001b[39m.\u001b[39mfit()\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(logit_result\u001b[39m.\u001b[39msummary())\n",
      "File \u001b[0;32m~/miniconda3/envs/py_dl/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:465\u001b[0m, in \u001b[0;36mBinaryModel.__init__\u001b[0;34m(self, endog, exog, check_rank, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, MultinomialModel):\n\u001b[1;32m    464\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendog \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendog \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m--> 465\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mendog must be in the unit interval.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    467\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_continuous_ok \u001b[39mand\u001b[39;00m\n\u001b[1;32m    468\u001b[0m             np\u001b[39m.\u001b[39many(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendog \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mround(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendog))):\n\u001b[1;32m    469\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mendog must be binary, either 0 or 1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: endog must be in the unit interval."
     ]
    }
   ],
   "source": [
    "# logistic regresstion - maximizenp\n",
    "# input: features output:\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "y_pred = sm.add_constant(df)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logit_model = sm.Logit(y_target, y_pred)\n",
    "logit_result = logit_model.fit()\n",
    "print(logit_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY THIS AS WELL\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# hyperparameters to play with: \n",
    "# 1) number of layers\n",
    "# 2) number of neurons per layer\n",
    "# 3) activation function\n",
    "# 4) optimizer\n",
    "# 5) learning rate\n",
    "# 6) batch size\n",
    "# 7) number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cut seg from RNAcomplete file into all shifts of len 20\n",
    "# shifts_RNAcompete_master_list = np.empty((O), dtype=object) # sequences\n",
    "\n",
    "\n",
    "# with open(file_path, 'r') as file:\n",
    "#         for n, line in enumerate(file):\n",
    "#             line = line.rstrip('\\n')  # Remove newline character from the end\n",
    "#             line_length = len(line)\n",
    "#             if line_length > 20: # create k-mers of all possible shifts \n",
    "#                 shifts = np.empty((line_length - 20 + 1), dtype=f'|S{L_RBNS}')\n",
    "#                 for i in range(line_length - 20 + 1):\n",
    "#                     shifts[i] = line[i:i+20]\n",
    "#                     #shifts_RNAcompete_master_list[n] = line[i:i+20]\n",
    "#                     # n += 1\n",
    "#                 shifts_RNAcompete_master_list[n] = shifts\n",
    "#             else:\n",
    "#                 shifts_RNAcompete_master_list[n] = line\n",
    "                \n",
    "# shifts_RNAcompete_master_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input to NN shifts_RNAcompete_master_list sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) take all sequences from file RNAcompete_2009_dataset.txt and input into NNcomplete_sequnces.txt (seq len 30-40)\n",
    "# create all possible k-mers with shift 1 (len 20) \n",
    "# input into NN\n",
    "# choose max and min (features) probability for each full sequence - find best k-mer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) model feature to find RNCMPT_train on all proteins\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
