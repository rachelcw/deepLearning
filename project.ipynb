{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load packages env py_dl\n",
    "import os\n",
    "# os.environ['OMP_NUM_THREADS'] = '50'\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv1D\n",
    "from keras import initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) constants and hyperparameters\n",
    "L_RBNS = 20 # length of each sequence in RBNS data\n",
    "O = int(1e7) # for initializing big arrays, helps reduce runtime\n",
    "LIMIT_FILE_N_SEQ_READ = int(1e6) # limit the amount of seq we read from file, helps reduce runtime\n",
    "ONE_HOT_DICT = {b'A': np.array([0,0,0,1], dtype=np.float16),\n",
    "                b'C': np.array([0.,0.,1,0], dtype=np.float16),\n",
    "                b'G': np.array([0,1,0,0], dtype=np.float16),\n",
    "                b'T': np.array([1,0,0,0], dtype=np.float16),\n",
    "                b'U': np.array([1,0,0,0], dtype=np.float16),\n",
    "                b'N': np.array([0.25,0.25,0.25,0.25])\n",
    "                }\n",
    "\n",
    "ONE_HOT_DICT2 = {'A':[0,0,0,1],\n",
    "                'C': [0,0,1,0],\n",
    "                'G':[0,1,0,0],\n",
    "                'T': [1,0,0,0],\n",
    "                'U': [1,0,0,0],\n",
    "                'N': [0.25,0.25,0.25,0.25]\n",
    "                }\n",
    "\n",
    "L_RBNS = 20 # length of each sequence in RBNS data\n",
    "FILES_40 = [\"RBP9\" , \"RBP11\", \"RBP15\", \"RBP31\"] # files  with sequences len 40 and not 20\n",
    "\n",
    "model_param_dict = {\"kernel_size\":3, \"pool_size\":2, \"layers\": [128, 128], \"final_activation_function\":\"sigmoid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBP11_1090nM.seq', 'RBP11_121nM.seq', 'RBP11_13nM.seq', 'RBP11_1nM.seq', 'RBP11_3280nM.seq', 'RBP11_365nM.seq', 'RBP11_40nM.seq', 'RBP11_4nM.seq', 'RBP11_9800nM.seq', 'RBP11_input.seq']\n",
      "['RBP11_input.seq', 'RBP11_1nM.seq', 'RBP11_4nM.seq', 'RBP11_13nM.seq', 'RBP11_1090nM.seq', 'RBP11_3280nM.seq', 'RBP11_9800nM.seq']\n"
     ]
    }
   ],
   "source": [
    "# 3) read files \n",
    "# list files in RBNS_training\n",
    "directory = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training'\n",
    "RBNS_training_files = []\n",
    "for filename in os.listdir(directory): # Iterate over all files in the directory\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        RBNS_training_files.append(filename)\n",
    "# print(RBNS_training_files)\n",
    "\n",
    "# list files in single protein - filter out protein 1 files only\n",
    "PROTEIN = \"RBP11\" ### TODO change \n",
    "filtered_list = [value for value in RBNS_training_files if value.startswith(str(PROTEIN)+'_')]\n",
    "print(filtered_list)\n",
    "\n",
    "# order file names according to concentration\n",
    "modified_list = [value.replace(f'{PROTEIN}_', '').replace('nM.seq', '') for value in filtered_list]\n",
    "if f'{PROTEIN}_input.seq' in filtered_list: # get rid of input so only ints are left\n",
    "    modified_list.remove('input.seq')\n",
    "modified_list = sorted(modified_list, key=int)\n",
    "modified_list = modified_list[:3]+modified_list[-3:]\n",
    "modified_list = [PROTEIN+\"_\"+str(modified_list[i])+\"nM.seq\" for i in range(len(modified_list))]\n",
    "modified_list.insert(0,str(PROTEIN)+\"_input.seq\")\n",
    "print(modified_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) MULTICALSS classification - \n",
    "# initalize np.arrays\n",
    "N = LIMIT_FILE_N_SEQ_READ * len(modified_list) # number of seqs\n",
    "master_list = np.empty((N), dtype=f'|S{L_RBNS}') # sequences\n",
    "class_lables = np.zeros((N, len(modified_list)), dtype=np.int8) # array of probolities per class\n",
    "class_lables[:, 0] = 1 # set all to 1 for the first class (input.seq)\n",
    "for i, file in enumerate(modified_list): # set the lables for the rest of the classes\n",
    "    if file != str(PROTEIN + '_input.seq'):\n",
    "        class_lables[LIMIT_FILE_N_SEQ_READ * i: LIMIT_FILE_N_SEQ_READ * (i + 1), i:] = 1\n",
    "n = 0 # index for master_list\n",
    "## running on protein 1 files only ############################################################\n",
    "for file_index, file in enumerate(modified_list):\n",
    "    #print(file_index)\n",
    "    file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RBNS_training/' + file\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [f.readline() for _ in range(LIMIT_FILE_N_SEQ_READ)] # read LIMIT_FILE_N_SEQ_READ lines\n",
    "    # cut seq to 20 length randomly if len is longer find a random starting index and take 20 from there\n",
    "    if file in FILES_40: # file len 40\n",
    "        # choose random index to start from and select 20 chars - save shortend seq into master_list\n",
    "        start_index = random.randint(0, 20)\n",
    "        for line in lines:\n",
    "            seq = line.split('\\t')[0] # take only the sequence\n",
    "            master_list[n] = seq[start_index : start_index + 20] # shortened seq\n",
    "            n += 1\n",
    "    else: # file len 20 + isnt input.seq\n",
    "        for line in lines:\n",
    "            seq = line.split('\\t')[0] # take only the sequence\n",
    "            master_list[n] = seq # seq\n",
    "            n += 1\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "random.seed(123)\n",
    "rand_master_list = np.empty((LIMIT_FILE_N_SEQ_READ * 2), dtype=f'|S{L_RBNS}') # sequences\n",
    "rand_class_labels = np.zeros((LIMIT_FILE_N_SEQ_READ * 2, len(modified_list)), dtype=np.int8)\n",
    "ABC = [b'A', b'C', b'G', b'T']\n",
    "rand_master_list = np.array([b''.join([ABC[random.randint(0,3)] for _ in range(L_RBNS)]) for _ in range(LIMIT_FILE_N_SEQ_READ * 2)], dtype=f'|S{L_RBNS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_lables = np.concatenate((class_lables, rand_class_labels))\n",
    "master_list = np.concatenate((master_list, rand_master_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_SIZE = model_param_dict[\"kernel_size\"] - 1\n",
    "SEQ_PADDED_LEN = 20 + 2 * PAD_SIZE # TODO: change 20 to 20 or 40\n",
    "one_hot = np.array([[ONE_HOT_DICT[bytes([nuc])] for nuc in (b\"N\" * PAD_SIZE + seq + b\"N\" * PAD_SIZE)] for seq in master_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) NN per protien - input each seq (len 20) in master_list into NN \n",
    "    # output = vec of probabilities (one per each concentration)\n",
    "    # compare output to bool numpy array (true lable)\n",
    "    # backpropogation to model to minimize loss?\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the metric for model\n",
    "def accuracy_th(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Cast labels to float32\n",
    "    threshold = 0.5\n",
    "    y_pred_thresholded = tf.where(y_pred >= threshold, 1., 0.)  # Apply the threshold to the predicted probabilities\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred_thresholded), tf.float32))  # Calculate the accuracy\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the layer size is: 128\n",
      "the layer size is: 128\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 22, 128)           1664      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 7, 128)            49280     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 896)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 896)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               114816    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,046\n",
      "Trainable params: 183,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 22:53:40.574429: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1954/1954 [==============================] - 234s 119ms/step - loss: 0.5777 - accuracy_th: 0.6970 - val_loss: 0.5752 - val_accuracy_th: 0.6987\n",
      "Epoch 2/3\n",
      "1954/1954 [==============================] - 224s 115ms/step - loss: 0.5757 - accuracy_th: 0.6983 - val_loss: 0.5752 - val_accuracy_th: 0.6987\n",
      "Epoch 3/3\n",
      "1954/1954 [==============================] - 220s 113ms/step - loss: 0.5755 - accuracy_th: 0.6984 - val_loss: 0.5752 - val_accuracy_th: 0.6989\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Unable to create file (unable to open file: name = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Compile the model with Optimizer, Loss Function and Metrics\u001b[39;00m\n\u001b[1;32m     40\u001b[0m run_hist_1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X, Y, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, epochs\u001b[39m=\u001b[39mEPHOCHS, validation_split\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_multiprocessing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, workers\u001b[39m=\u001b[39m\u001b[39m90\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/h5py/_hl/files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[1;32m    525\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    527\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    528\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    529\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    530\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    531\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    532\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 533\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39mswmr)\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/h5py/_hl/files.py:232\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    230\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mcreate(name, h5f\u001b[39m.\u001b[39mACC_EXCL, fapl\u001b[39m=\u001b[39mfapl, fcpl\u001b[39m=\u001b[39mfcpl)\n\u001b[1;32m    231\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 232\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mcreate(name, h5f\u001b[39m.\u001b[39mACC_TRUNC, fapl\u001b[39m=\u001b[39mfapl, fcpl\u001b[39m=\u001b[39mfcpl)\n\u001b[1;32m    233\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    234\u001b[0m     \u001b[39m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:126\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Unable to create file (unable to open file: name = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "\n",
    "output_size = 6 # number of classes\n",
    "X = one_hot\n",
    "Y = class_lables\n",
    "\n",
    "#  CNN\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=model_param_dict[\"kernel_size\"], strides=1,\n",
    "                 kernel_initializer=initializers.RandomNormal(stddev=0.01), activation='relu',\n",
    "                 input_shape=X.shape[1:], use_bias=True, bias_initializer='RandomNormal'))\n",
    "model.add(Conv1D(filters=128, kernel_size=model_param_dict[\"kernel_size\"], strides=3))\n",
    "# model.add(MaxPooling1D(pool_size=model_param_dict[\"pool_size\"], strides=None, padding='valid', data_format='channels_last')) # MAYBE USE CONV KERNEL HERE INSTEAD \n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "# per layer\n",
    "for layer_size in model_param_dict['layers']:\n",
    "            print(f'the layer size is: {layer_size}')\n",
    "            model.add(Dense(layer_size, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "model.add(Dense(output_size, activation=model_param_dict['final_activation_function'])) \n",
    "\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# Hyperparameters\n",
    "EPHOCHS = 3\n",
    "model.compile(optimizer=Adam(learning_rate = .003), loss=\"binary_crossentropy\", metrics=[accuracy_th])\n",
    "model.summary()\n",
    "\n",
    "# shuffle data\n",
    "rng = np.random.default_rng(32)\n",
    "shuf_inds = rng.permutation(X.shape[0])\n",
    "X = X[shuf_inds][:int(1e6)]\n",
    "Y = Y[shuf_inds][:int(1e6)]\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "run_hist_1 = model.fit(X, Y, batch_size=256, epochs=EPHOCHS, validation_split=0.5, shuffle=True, use_multiprocessing=True, workers=90)\n",
    "model.save('/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5') # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 12:01:56.476668: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 12:01:56.498853: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = keras.models.load_model(\"/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/model_multiclass.h5\", custom_objects={\"accuracy_th\":accuracy_th})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) # cut seg from RNAcomplete file into all shifts of len 24\n",
    "# read txt file\n",
    "file_path = '/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNAcompete_sequences.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # convert seqences to one hot encoding + pad seq \n",
    "    RNAcompete_master_list_one_hot = [[ONE_HOT_DICT2[nuc] for nuc in ((41 - len(seq.rstrip('\\n'))) * 'N' + seq.rstrip('\\n'))] for seq in file if len(seq) > 5]\n",
    "num_seqs = len(RNAcompete_master_list_one_hot)\n",
    "\n",
    "# SHIFTS - RNAcomplete file into all shifts of len 24 -- create k-mers of all possible shifts \n",
    "shifts_RNAcompete_master_list = [[seq[i:i+SEQ_PADDED_LEN] for i in range(0, len(seq) - SEQ_PADDED_LEN + 1, 5)] for seq in RNAcompete_master_list_one_hot] # step +5 \n",
    "shifts_RNAcompete_master_list = np.reshape(shifts_RNAcompete_master_list, (-1, SEQ_PADDED_LEN, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30170/30170 [==============================] - 216s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# create features: MULTICLASS parallelize the prediction + output -  suppose predict gives N dim vector:\n",
    "N = 6 # number of classes\n",
    "prediction = model.predict(shifts_RNAcompete_master_list, use_multiprocessing=True, workers=90)\n",
    "prediction = np.reshape(prediction, (num_seqs, -1, N))\n",
    "# fetures - 6 max + 6 min per seq with best shift\n",
    "features = np.empty((num_seqs, 2 * N))\n",
    "features[:, :N] = np.max(prediction, axis=1) # shape is (num_seqs, N)\n",
    "features[:, N:] = np.min(prediction, axis=1) # shape is (num_seqs, N)\n",
    "\n",
    "# # write pred into file\n",
    "# protein=\"RBP1\"\n",
    "# with open(f'/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/max_predicitions_{protein}.txt', \"w\") as file:\n",
    "#     for pred in max_prediction:\n",
    "#         file.write(f'{pred}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27241164380909\n"
     ]
    }
   ],
   "source": [
    "# # LinearRegression - between y_hat and y_target\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# X = np.array(features)  # Independent variable (features)\n",
    "\n",
    "# with open('/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNCMPT_training/RBP1.txt','r') as file:\n",
    "#     y_target=[float(line.strip()) for line in file if line != \"\"] ### y\n",
    "\n",
    "# y = np.array(y_target)  # Dependent variable (target)\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(X, y)\n",
    "# y_pred = model.predict(X)\n",
    "\n",
    "# from scipy.stats import pearsonr\n",
    "# correlation, _ = pearsonr(y, y_pred)\n",
    "# print(correlation)\n",
    "\n",
    "# correlation_coefficient, p_value = stats.pearsonr(y_target, features_max)\n",
    "# print(\"Pearson correlation coefficient:\", correlation_coefficient, \"p-value:\", p_value)\n",
    "\n",
    "# # Print the coefficients (slope and intercept)\n",
    "# print(\"Coefficients:\", model.coef_)     # Slope\n",
    "# print(\"Intercept:\", model.intercept_)   # Intercept\n",
    "# # print(\"Maximum:\", max(y_pred))\n",
    "# # print(\"Minimum:\", min(y_pred))\n",
    "\n",
    "# # Visualize the linear regression line\n",
    "# plt.scatter(X, y) \n",
    "# plt.plot(X, y_pred, color='red', label='Linear Regression') ## model\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Formula:\n",
      "y = 1.1906 + -15.2344 + 20.3540 + -9.8116 + -9.1625 + -1.7815 + 24.2165 + -4.4641 + -15.8454 + 11.6378 + -11.0284 + 2.0439 + 8.7680\n"
     ]
    }
   ],
   "source": [
    "# # LinearRegression - Get the coefficients and the intercept\n",
    "# coefficients = model.coef_\n",
    "# intercept = model.intercept_\n",
    "\n",
    "# # Create the formula for the Linear Regression model\n",
    "# formula_parts = [f\"{coeff:.4f}\" for coeff in coefficients]\n",
    "# formula = f\"y = {intercept:.4f} + \" + \" + \".join(formula_parts)\n",
    "\n",
    "# print(\"Linear Regression Formula:\")\n",
    "# print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "943/943 [==============================] - 2s 2ms/step - loss: 0.6517 - pearson_correlation: 0.3483 - val_loss: 0.6263 - val_pearson_correlation: 0.3737\n",
      "Epoch 2/5\n",
      "943/943 [==============================] - 1s 1ms/step - loss: 0.6254 - pearson_correlation: 0.3746 - val_loss: 0.6210 - val_pearson_correlation: 0.3790\n",
      "Epoch 3/5\n",
      "943/943 [==============================] - 4s 4ms/step - loss: 0.6231 - pearson_correlation: 0.3769 - val_loss: 0.6189 - val_pearson_correlation: 0.3811\n",
      "Epoch 4/5\n",
      "943/943 [==============================] - 5s 5ms/step - loss: 0.6219 - pearson_correlation: 0.3781 - val_loss: 0.6155 - val_pearson_correlation: 0.3845\n",
      "Epoch 5/5\n",
      "943/943 [==============================] - 5s 5ms/step - loss: 0.6183 - pearson_correlation: 0.3817 - val_loss: 0.6171 - val_pearson_correlation: 0.3829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d8c9eb640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data01/private/resources/RACHELI_EDEN_SHARED/DL_PROJ/RNCMPT_training/RBP1.txt','r') as file:\n",
    "    y_target=np.array([float(line.strip()) for line in file if line != \"\"])\n",
    "\n",
    "# # Create a Sequential model with 1 output neurons for Pearson correlation\n",
    "output_neurons = 1\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='linear', input_shape=(features.shape[1],))) # single neuron layer with 'linear' activation (no activation)\n",
    "\n",
    "# Define the custom Pearson correlation coefficient loss function\n",
    "def pearson_correlation_loss(y_true, y_pred):\n",
    "    return 1 - pearson_correlation(y_true, y_pred)  # Return 1 - correlation to minimize the negative correlation\n",
    "\n",
    "def pearson_correlation(y_true, y_pred):\n",
    "    y_true_mean = tf.reduce_mean(y_true)\n",
    "    y_pred_mean = tf.reduce_mean(y_pred)\n",
    "    covariance = tf.reduce_mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    y_true_std = tf.math.reduce_std(y_true)\n",
    "    y_pred_std = tf.math.reduce_std(y_pred)\n",
    "    correlation = covariance / (y_true_std * y_pred_std)\n",
    "    return correlation  # Return 1 - correlation to minimize the negative correlation\n",
    "\n",
    "# Compile model with the custom loss function\n",
    "model.compile(optimizer='adam', loss=pearson_correlation_loss, metrics=[pearson_correlation])\n",
    "# Fit the model to the data\n",
    "model.fit(features, y_target, batch_size=128, epochs=5, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Formula:\n",
      "y = -0.011234917677938938 (b) + [0.10428984] + [0.501944] + [-0.3563016] + [-0.52175224] + [-0.59814584] + [0.7204223] + [0.18594183] + [0.04441676] + [-0.46729964] + [-0.7657383] + [0.30190748] + [0.11686829]\n"
     ]
    }
   ],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "# Create the formula for the Linear Regression model\n",
    "formula_parts = [f\"{coeff}\" for coeff in weights]\n",
    "formula = f\"y = {biases[0]} (b) + \" + \" + \".join(formula_parts)\n",
    "\n",
    "print(\"NN Formula:\")\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) model feature to find RNCMPT_train on all proteins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
